{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Randhawa\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import pandas\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = pandas.read_csv('FinalData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#These values do not affect what we are predicting (not even referee in my opinion) so we will delete them\n",
    "del dataset['ID']\n",
    "del dataset['Date']\n",
    "del dataset['Referee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These are neither getting predicted or providing prior knowledge\n",
    "del dataset['HTHG']\n",
    "del dataset['HTAG']\n",
    "del dataset['HTR-A']\n",
    "del dataset['HTR-D']\n",
    "del dataset['HTR-H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalisation. Dividing wins by number of games played to make it even for all teams. Can't do 0 divided by 0 so keep the same if less than 1\n",
    "dataset['HHWins'] = np.where(dataset['HHGames'] < 1, dataset['HHWins'], dataset['HHWins']/dataset['HHGames'])\n",
    "dataset['AHWins'] = np.where(dataset['AHGames'] < 1, dataset['AHWins'], dataset['AHWins']/dataset['AHGames'])\n",
    "dataset['HAWins'] = np.where(dataset['HAGames'] < 1, dataset['HAWins'], dataset['HAWins']/dataset['HAGames'])\n",
    "dataset['AAWins'] = np.where(dataset['AAGames'] < 1, dataset['AAWins'], dataset['AAWins']/dataset['AAGames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encode the strings as numericals in order for the algorithms to be able to take them\n",
    "#0-Away win, 1- Draw, 2- Home Win\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl_enc = LabelEncoder()\n",
    "lbl_enc.fit(dataset.FTR)\n",
    "dataset.FTR = lbl_enc.transform(dataset.FTR)\n",
    "lbl_enc.fit(dataset.HomeTeam)\n",
    "dataset.HomeTeam = lbl_enc.transform(dataset.HomeTeam)\n",
    "lbl_enc.fit(dataset.AwayTeam)\n",
    "dataset.AwayTeam = lbl_enc.transform(dataset.AwayTeam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTHG</th>\n",
       "      <th>FTAG</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>HST</th>\n",
       "      <th>AST</th>\n",
       "      <th>HF</th>\n",
       "      <th>AF</th>\n",
       "      <th>...</th>\n",
       "      <th>HHCorners</th>\n",
       "      <th>HAShots</th>\n",
       "      <th>HAShotsTarget</th>\n",
       "      <th>HACorners</th>\n",
       "      <th>AHShots</th>\n",
       "      <th>AHShotsTarget</th>\n",
       "      <th>AHCorners</th>\n",
       "      <th>AAShots</th>\n",
       "      <th>AAShotsTarget</th>\n",
       "      <th>AACorners</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.927944</td>\n",
       "      <td>8.571178</td>\n",
       "      <td>0.602812</td>\n",
       "      <td>2.233743</td>\n",
       "      <td>12.430580</td>\n",
       "      <td>12.776801</td>\n",
       "      <td>4.797891</td>\n",
       "      <td>6.126538</td>\n",
       "      <td>11.404218</td>\n",
       "      <td>11.611599</td>\n",
       "      <td>...</td>\n",
       "      <td>5.069872</td>\n",
       "      <td>9.603560</td>\n",
       "      <td>4.189039</td>\n",
       "      <td>4.165731</td>\n",
       "      <td>9.853875</td>\n",
       "      <td>4.291985</td>\n",
       "      <td>4.242475</td>\n",
       "      <td>10.770752</td>\n",
       "      <td>4.858503</td>\n",
       "      <td>4.576006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.856863</td>\n",
       "      <td>9.850980</td>\n",
       "      <td>1.001961</td>\n",
       "      <td>1.001961</td>\n",
       "      <td>13.556863</td>\n",
       "      <td>11.074510</td>\n",
       "      <td>5.845098</td>\n",
       "      <td>4.831373</td>\n",
       "      <td>11.354902</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.336828</td>\n",
       "      <td>10.425229</td>\n",
       "      <td>4.679287</td>\n",
       "      <td>4.365888</td>\n",
       "      <td>10.555834</td>\n",
       "      <td>4.775979</td>\n",
       "      <td>4.483465</td>\n",
       "      <td>10.096011</td>\n",
       "      <td>4.594728</td>\n",
       "      <td>4.282331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.030837</td>\n",
       "      <td>10.522026</td>\n",
       "      <td>2.346916</td>\n",
       "      <td>0.484581</td>\n",
       "      <td>14.929515</td>\n",
       "      <td>9.914097</td>\n",
       "      <td>7.429515</td>\n",
       "      <td>4.213656</td>\n",
       "      <td>10.986784</td>\n",
       "      <td>11.856828</td>\n",
       "      <td>...</td>\n",
       "      <td>5.684207</td>\n",
       "      <td>11.149861</td>\n",
       "      <td>5.150235</td>\n",
       "      <td>4.730764</td>\n",
       "      <td>10.955354</td>\n",
       "      <td>5.022493</td>\n",
       "      <td>4.648573</td>\n",
       "      <td>9.707656</td>\n",
       "      <td>4.356117</td>\n",
       "      <td>4.128744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HomeTeam   AwayTeam      FTHG      FTAG         HS         AS       HST  \\\n",
       "FTR                                                                             \n",
       "0    10.927944   8.571178  0.602812  2.233743  12.430580  12.776801  4.797891   \n",
       "1     9.856863   9.850980  1.001961  1.001961  13.556863  11.074510  5.845098   \n",
       "2     9.030837  10.522026  2.346916  0.484581  14.929515   9.914097  7.429515   \n",
       "\n",
       "          AST         HF         AF    ...      HHCorners    HAShots  \\\n",
       "FTR                                    ...                             \n",
       "0    6.126538  11.404218  11.611599    ...       5.069872   9.603560   \n",
       "1    4.831373  11.354902  12.000000    ...       5.336828  10.425229   \n",
       "2    4.213656  10.986784  11.856828    ...       5.684207  11.149861   \n",
       "\n",
       "     HAShotsTarget  HACorners    AHShots  AHShotsTarget  AHCorners    AAShots  \\\n",
       "FTR                                                                             \n",
       "0         4.189039   4.165731   9.853875       4.291985   4.242475  10.770752   \n",
       "1         4.679287   4.365888  10.555834       4.775979   4.483465  10.096011   \n",
       "2         5.150235   4.730764  10.955354       5.022493   4.648573   9.707656   \n",
       "\n",
       "     AAShotsTarget  AACorners  \n",
       "FTR                            \n",
       "0         4.858503   4.576006  \n",
       "1         4.594728   4.282331  \n",
       "2         4.356117   4.128744  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby('FTR').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for an away win, there are usually more away corners, shots and shots on target. So there is definitely a correlation between the team that wins and these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x188a0f57cc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGHCAYAAACXsdlkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLFwkSVJjWLhIkqTGsHCRJEmN8f8DLoCANMD9NaMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1889bf75940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# histogram of results\n",
    "dataset[\"FTR\"].hist()\n",
    "plt.title('Histogram of Results')\n",
    "plt.xlabel('Result')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the histograms, a home win is nearly twice as frequent as an away win in the dataset, and home wins occur nearly as often as away wins and draws put together. This suggests the team playing at home hold some sort of advantage over the away team. So it is necessary to separate the features into home and away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHODOLOGY OVERVIEW\n",
    "\n",
    "In order to gain some insight into how this problem could be approached, I initially had a look around to see how others have attempted this sort of thing in the past. I came accross a paper 'Predicting Soccer Match Results in the English Premier League' by Ben Ulmer and Matthew Fernandez, in which they used the concept of a team's 'form' to predict results. They did this by looking at a team's past 10 matches, and how many times they won in this period. This seems like a plausible method, however I did not want to ignore 15 seasons worth of data and so I decided to use the idea of form on the whole training set. I recognise that form does not crossover from season to season, as major changes take place over the summer for every team, so I decided to split the data by seasons and calculate form within each season. To calculate form, instead of only looking at the past X games, I used data from all the games to calculate cumulative totals for each team based the number of wins that season, shots, shots on target, and corners. I did not feel that fouls, yellow cards and red cards reflected the strength of a team, as some very successful teams have their game based around slowing the match down by fouling the oppostion constantly.\n",
    "\n",
    "I had originally thought of getting all the results for every team since 2003, and also for the lower leagues and making one big file with all these matches, and thengrouping the matches by season and then by team, but I decided to stick with the matches provided in the training set. I divided the number of wins for each team by the number of games played, because some teams had not played anywhere near as many matches as other teams in the dataset, and so this was a way of evening things out.\n",
    "\n",
    "The target variable, FTR, could have been dummy coded into different binary variables, but then predicting this would have involved multi-label classification algorithms which is still a new field of research. I decided to keep it as a categorical variable with three classes, and convert the letters into numericals, as this would make it easier to feed the dataset to algorithms. If there were many more classes, this may have caused bias issues, but three is a small enough number to make this work. i decided to go with Logistic Regression because it is perfectly suited to predicting categoric variables and performed at a higher accuracy than other algorithms I tested such as decision forests. I split the dataset into a 70 percent to 30 percent ratio for training to test sets, because I found this prevents overfitting of the data, which would cause classifier bias.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all the columns from the dataframe.\n",
    "columns = dataset.columns.tolist()\n",
    "# Filter the columns to remove ones we don't want.\n",
    "columns = [c for c in columns if c not in [\"Season\",\"FTHG\", \"FTAG\", \"FTR\", \"HS\", \"AS\", \"HST\", \"HST\", \"AST\", \"HF\", \"AF\", \"HC\", \"AC\", \"HY\", \"AY\", \"HR\", \"AR\", \"AS\"]]\n",
    "\n",
    "# Store the variable we'll be predicting on.\n",
    "target = \"FTR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1391, 38)\n",
      "(596, 38)\n"
     ]
    }
   ],
   "source": [
    "# Generate the training set.  Set random_state to be able to replicate results. 0.7 to not overfit data.\n",
    "train = dataset.sample(frac=0.7, random_state=1)\n",
    "# Select anything not in the training set and put it in the testing set.\n",
    "test = dataset.loc[~dataset.index.isin(train.index)]\n",
    "# Print the shapes of both sets.\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model class.\n",
    "model = LogisticRegression()\n",
    "# Fit the model to the training data.\n",
    "model.fit(train[columns], train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52624011502516177"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy\n",
    "model.score(train[columns], train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is very high at 53%,almost as good as the bookies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2332214765100671"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the scikit-learn function to compute error.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Generate our predictions for the test set.\n",
    "predictions = model.predict(test[columns])\n",
    "\n",
    "# Compute error between our test predictions and the actual values.\n",
    "mean_squared_error(predictions, test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1718188353702372"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean\n",
    "train[target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 0 2 2 2 2 2 2 0 0 2 2 0 2 2 2 2 2 2 2 1 0 2 0 2 2 2 0 2 0 2 2 2 2 2\n",
      " 2 2 0 2 2 2 2 2 2 2 2 2 2 0 2 0 2 0 0 2 2 0 2 2 2 2 2 0 0 2 0 0 0 0 2 2 0\n",
      " 0 2 2 2 2 2 0 0 2 2 2 0 2 2 0 2 2 2 2 0 0 0 2 2 2 2 2 2 0 0 0 2 2 0 0 0 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 0 2 0 2 0 0 2 0 0 2 2 2 2 2 2 0 2 2 2 2 2 0\n",
      " 2 0 2 2 2 0 2 2 2 2 0 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 0\n",
      " 2 0 0 2 0 2 2 2 2 2 0 2 2 0 0 2 2 2 2 2 2 0 2 0 2 2 0 0 0 2 2 2 2 2 0 2 2\n",
      " 2 0 2 2 0 0 0 2 2 2 2 2 2 0 2 0 2 0 2 2 2 0 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2\n",
      " 2 0 2 0 1 2 0 2 2 2 2 2 2 2 2 0 2 2 2 0 2 2 2 2 2 2 2 2 2 0 2 2 2 2 0 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 0 2 2 2 2 0 2 0 2 2 0 2 2\n",
      " 2 0 2 0 1 2 2 2 2 2 0 2 0 2 2 0 2 2 2 0 0 2 2 0 2 2 2 2 0 2 0 2 2 2 2 0 2\n",
      " 2 2 0 2 0 2 2 0 0 0 2 2 0 2 0 2 2 0 0 2 2 2 0 0 2 0 0 2 2 2 2 2 2 0 0 0 2\n",
      " 0 0 2 2 2 2 0 0 2 2 2 0 0 2 2 2 0 0 0 2 2 2 2 2 2 0 2 0 2 0 0 0 0 2 2 0 2\n",
      " 2 2 2 0 0 2 2 2 2 2 0 2 2 0 2 0 0 2 2 2 2 2 2 0 0 0 0 2 2 2 0 2 2 1 2 2 0\n",
      " 2 0 0 2 2 0 2 0 2 0 0 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 0 2 2 2 0 0 0 2 2 0 0\n",
      " 0 0 2 2 2 2 2 0 2 2 0 2 2 2 0 0 0 2 0 2 2 2 2 2 0 2 0 0 2 2 0 0 2 2 2 2 0\n",
      " 2 0 2 2 0 2 0 0 2 0 2 2 0 2 2 2 0 2 2 2 1 2 1 0 2 2 2 2 2 2 0 2 2 2 0 2 2\n",
      " 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.52778272  0.2370725   0.23514478]\n",
      " [ 0.15807644  0.25797846  0.5839451 ]\n",
      " [ 0.44831844  0.30265654  0.24902501]\n",
      " ..., \n",
      " [ 0.08391621  0.18279973  0.73328406]\n",
      " [ 0.36456622  0.23456401  0.40086976]\n",
      " [ 0.21032778  0.26331502  0.5263572 ]]\n"
     ]
    }
   ],
   "source": [
    "# generate class probabilities\n",
    "probs = model.predict_proba(test[columns])\n",
    "print (probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the classifier is predicting a 0 (away win) any time the probability is greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51428571  0.54285714  0.55        0.46428571  0.43884892  0.53956835\n",
      "  0.53956835  0.51798561  0.50724638  0.45255474]\n",
      "0.506720091579\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using 10-fold cross-validation\n",
    "scores = cross_val_score(LogisticRegression(), train[columns], train[target], scoring='accuracy', cv=10)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 10-fold cross-validation, the accuracy of 53% still holds, which is a good sign.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results and Prediction for weekend fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "testset = pandas.read_csv('TestEPLData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do all the same transformation\n",
    "del testset['ID']\n",
    "del testset['Date']\n",
    "del testset['Referee']\n",
    "del testset['HTHG']\n",
    "del testset['HTAG']\n",
    "del testset['HTR-A']\n",
    "del testset['HTR-D']\n",
    "del testset['HTR-H']\n",
    "del testset['FTHG']\n",
    "del testset['FTAG']\n",
    "del testset['HS']\n",
    "del testset['AS']\n",
    "del testset['HST']\n",
    "del testset['AST']\n",
    "del testset['HF']\n",
    "del testset['AF']\n",
    "del testset['HC']\n",
    "del testset['AC']\n",
    "del testset['HY']\n",
    "del testset['AY']\n",
    "del testset['HR']\n",
    "del testset['AR']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "testset['HHWins'] = np.where(testset['HHGames'] < 1, testset['HHWins'], testset['HHWins']/testset['HHGames'])\n",
    "testset['AHWins'] = np.where(testset['AHGames'] < 1, testset['AHWins'], testset['AHWins']/testset['AHGames'])\n",
    "testset['HAWins'] = np.where(testset['HAGames'] < 1, testset['HAWins'], testset['HAWins']/testset['HAGames'])\n",
    "testset['AAWins'] = np.where(testset['AAGames'] < 1, testset['AAWins'], testset['AAWins']/testset['AAGames'])\n",
    "lbl_enc = LabelEncoder()\n",
    "lbl_enc.fit(testset.FTR)\n",
    "testset.FTR = lbl_enc.transform(testset.FTR)\n",
    "lbl_enc.fit(testset.HomeTeam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>FTR</th>\n",
       "      <th>Season</th>\n",
       "      <th>HHGames</th>\n",
       "      <th>HAGames</th>\n",
       "      <th>AAGames</th>\n",
       "      <th>AHGames</th>\n",
       "      <th>HHWins</th>\n",
       "      <th>AHWins</th>\n",
       "      <th>...</th>\n",
       "      <th>HHCorners</th>\n",
       "      <th>HAShots</th>\n",
       "      <th>HAShotsTarget</th>\n",
       "      <th>HACorners</th>\n",
       "      <th>AHShots</th>\n",
       "      <th>AHShotsTarget</th>\n",
       "      <th>AHCorners</th>\n",
       "      <th>AAShots</th>\n",
       "      <th>AAShotsTarget</th>\n",
       "      <th>AACorners</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-2017</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>10.126185</td>\n",
       "      <td>17.283367</td>\n",
       "      <td>5.469031</td>\n",
       "      <td>8.212262</td>\n",
       "      <td>9.115458</td>\n",
       "      <td>3.210376</td>\n",
       "      <td>3.081837</td>\n",
       "      <td>12.192212</td>\n",
       "      <td>5.394159</td>\n",
       "      <td>4.929675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-2017</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>5.566333</td>\n",
       "      <td>9.691024</td>\n",
       "      <td>2.447625</td>\n",
       "      <td>4.970323</td>\n",
       "      <td>14.242259</td>\n",
       "      <td>5.998860</td>\n",
       "      <td>3.610158</td>\n",
       "      <td>6.690125</td>\n",
       "      <td>2.872768</td>\n",
       "      <td>5.313723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-2017</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>7.501354</td>\n",
       "      <td>11.991029</td>\n",
       "      <td>2.612860</td>\n",
       "      <td>7.385992</td>\n",
       "      <td>13.624285</td>\n",
       "      <td>4.635256</td>\n",
       "      <td>8.654320</td>\n",
       "      <td>8.048362</td>\n",
       "      <td>3.461584</td>\n",
       "      <td>2.060494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-2017</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.446224</td>\n",
       "      <td>8.282644</td>\n",
       "      <td>2.349026</td>\n",
       "      <td>4.103347</td>\n",
       "      <td>17.548431</td>\n",
       "      <td>2.096058</td>\n",
       "      <td>4.031079</td>\n",
       "      <td>12.970043</td>\n",
       "      <td>3.799455</td>\n",
       "      <td>5.209236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-2017</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.641144</td>\n",
       "      <td>8.345680</td>\n",
       "      <td>4.462204</td>\n",
       "      <td>2.934759</td>\n",
       "      <td>20.026353</td>\n",
       "      <td>7.432527</td>\n",
       "      <td>9.990788</td>\n",
       "      <td>17.261448</td>\n",
       "      <td>4.149488</td>\n",
       "      <td>6.069197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-2017</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>4.692164</td>\n",
       "      <td>14.533070</td>\n",
       "      <td>5.222151</td>\n",
       "      <td>5.276757</td>\n",
       "      <td>9.375095</td>\n",
       "      <td>2.658002</td>\n",
       "      <td>5.558000</td>\n",
       "      <td>8.915716</td>\n",
       "      <td>1.515408</td>\n",
       "      <td>3.826328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-2017</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>10.120587</td>\n",
       "      <td>13.838605</td>\n",
       "      <td>5.655460</td>\n",
       "      <td>4.241193</td>\n",
       "      <td>16.563944</td>\n",
       "      <td>6.733074</td>\n",
       "      <td>6.483788</td>\n",
       "      <td>19.096320</td>\n",
       "      <td>7.065241</td>\n",
       "      <td>8.135566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-2017</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>5.823245</td>\n",
       "      <td>17.437872</td>\n",
       "      <td>4.674764</td>\n",
       "      <td>5.223290</td>\n",
       "      <td>10.113379</td>\n",
       "      <td>1.829122</td>\n",
       "      <td>3.414344</td>\n",
       "      <td>6.203114</td>\n",
       "      <td>1.057577</td>\n",
       "      <td>3.536195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-2017</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>7.685137</td>\n",
       "      <td>19.625089</td>\n",
       "      <td>7.003345</td>\n",
       "      <td>6.294061</td>\n",
       "      <td>8.282382</td>\n",
       "      <td>2.902540</td>\n",
       "      <td>3.946533</td>\n",
       "      <td>9.554056</td>\n",
       "      <td>3.349026</td>\n",
       "      <td>4.221793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-2017</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>7.216262</td>\n",
       "      <td>12.414855</td>\n",
       "      <td>5.271132</td>\n",
       "      <td>4.012176</td>\n",
       "      <td>10.293520</td>\n",
       "      <td>2.532171</td>\n",
       "      <td>3.718757</td>\n",
       "      <td>8.158832</td>\n",
       "      <td>1.441738</td>\n",
       "      <td>5.231624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HomeTeam  AwayTeam  FTR     Season  HHGames  HAGames  AAGames  AHGames  \\\n",
       "0         8        15    0  2016-2017        2        5        4        4   \n",
       "1         1        17    0  2016-2017        4        4        4        4   \n",
       "2         4         5    0  2016-2017        4        4        4        4   \n",
       "3        11        19    0  2016-2017        4        4        4        4   \n",
       "4        13        10    0  2016-2017        4        4        3        4   \n",
       "5        18        14    0  2016-2017        4        4        4        4   \n",
       "6         9        16    0  2016-2017        4        4        4        4   \n",
       "7         0         2    0  2016-2017        4        4        3        5   \n",
       "8         3         6    0  2016-2017        4        4        4        4   \n",
       "9        12         7    0  2016-2017        4        4        4        4   \n",
       "\n",
       "   HHWins  AHWins    ...      HHCorners    HAShots  HAShotsTarget  HACorners  \\\n",
       "0    1.00    0.50    ...      10.126185  17.283367       5.469031   8.212262   \n",
       "1    0.75    0.50    ...       5.566333   9.691024       2.447625   4.970323   \n",
       "2    0.25    0.25    ...       7.501354  11.991029       2.612860   7.385992   \n",
       "3    0.00    0.00    ...       4.446224   8.282644       2.349026   4.103347   \n",
       "4    0.25    0.00    ...       6.641144   8.345680       4.462204   2.934759   \n",
       "5    0.25    0.25    ...       4.692164  14.533070       5.222151   5.276757   \n",
       "6    0.75    1.00    ...      10.120587  13.838605       5.655460   4.241193   \n",
       "7    0.75    0.40    ...       5.823245  17.437872       4.674764   5.223290   \n",
       "8    0.75    0.50    ...       7.685137  19.625089       7.003345   6.294061   \n",
       "9    0.50    0.25    ...       7.216262  12.414855       5.271132   4.012176   \n",
       "\n",
       "     AHShots  AHShotsTarget  AHCorners    AAShots  AAShotsTarget  AACorners  \n",
       "0   9.115458       3.210376   3.081837  12.192212       5.394159   4.929675  \n",
       "1  14.242259       5.998860   3.610158   6.690125       2.872768   5.313723  \n",
       "2  13.624285       4.635256   8.654320   8.048362       3.461584   2.060494  \n",
       "3  17.548431       2.096058   4.031079  12.970043       3.799455   5.209236  \n",
       "4  20.026353       7.432527   9.990788  17.261448       4.149488   6.069197  \n",
       "5   9.375095       2.658002   5.558000   8.915716       1.515408   3.826328  \n",
       "6  16.563944       6.733074   6.483788  19.096320       7.065241   8.135566  \n",
       "7  10.113379       1.829122   3.414344   6.203114       1.057577   3.536195  \n",
       "8   8.282382       2.902540   3.946533   9.554056       3.349026   4.221793  \n",
       "9  10.293520       2.532171   3.718757   8.158832       1.441738   5.231624  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HomeTeam', 'AwayTeam', 'FTR', 'Season', 'HHGames', 'HAGames',\n",
       "       'AAGames', 'AHGames', 'HHWins', 'AHWins', 'AAWins', 'HAWins', 'HHShots',\n",
       "       'HHShotsTarget', 'HHCorners', 'HAShots', 'HAShotsTarget', 'HACorners',\n",
       "       'AHShots', 'AHShotsTarget', 'AHCorners', 'AAShots', 'AAShotsTarget',\n",
       "       'AACorners'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all the columns from the dataframe.\n",
    "columns2 = testset.columns.tolist()\n",
    "# Filter the columns to remove ones we don't want.\n",
    "columns2 = [c for c in columns if c not in [\"Season\"]]\n",
    "\n",
    "# Store the variable we'll be predicting on.\n",
    "target2 = \"FTR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate our predictions for the test set.\n",
    "predictions2 = model.predict(testset[columns2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 0, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = A, 2 = H\n",
    "So the predictions are [H, H, H, A, A, H, H, H, H, H]\n",
    "\n",
    "    Date\tHomeTeam\t    AwayTeam     Result\n",
    "21/01/2017\tLiverpool\t    Swansea      H\n",
    "21/01/2017\tBournemouth\t    Watford      H\n",
    "21/01/2017\tCrystal Palace\tEverton      H\n",
    "21/01/2017\tMiddlesbrough\tWest Ham     A\n",
    "21/01/2017\tStoke\t        Man United   A\n",
    "21/01/2017\tWest Brom\t    Sunderland   H\n",
    "21/01/2017\tMan City\t    Tottenham    H\n",
    "22/01/2017\tArsenal\t        Burnley      H\n",
    "22/01/2017\tChelsea\t        Hull         H\n",
    "22/01/2017\tSouthampton\t    Leicester    H\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
